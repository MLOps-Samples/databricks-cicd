name: Build_Test_Deploy

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

  # Allows you to run this workflow manually from the Actions tab
  workflow_dispatch:

jobs:
  build:
    environment: Databricks_Azure_Test
    runs-on: ubuntu-latest
    env:
      DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
      DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
      DATABRICKS_CLUSTERID: ${{ secrets.DATABRICKS_CLUSTERID }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v2

      - name: Setup Python
        uses: actions/setup-python@v2.2.2
        with:
          python-version: 3.7

      - name: Setup Miniconda
        uses: conda-incubator/setup-miniconda@v2.1.1
        with:
          environment-file: environment.yml
          activate-environment: databricks_cicd
          python-version: 3.7

      - name: Lint
        shell: bash -l {0}
        run: |
          flake8
          black .

      - name: Run unit tests
        if: ${{ false }}
        shell: bash -l {0}
        run: |
          echo "{\"host\": \"${DATABRICKS_HOST}\",\"token\": \"${DATABRICKS_TOKEN}\",\"cluster_id\": \"${DATABRICKS_CLUSTERID}\"}" > ~/.databricks-connect
          pip install -e src/
          pytest --cache-clear --cov=src test/unittests 2>&1 | tee pytest-coverage.txt

      - name: Comment coverage for pull requests
        uses: coroo/pytest-coverage-commentator@v1.0.2
        with:
          pytest-coverage: pytest-coverage.txt

      - name: Run integration tests
        env:
          DATABRICKS_DBFS_PATH: dbfs:/mnt/databrickscicd/Testing
          DATABRICKS_WORKSPACE_PATH: /databrickscicd/Testing
          DATABRICKS_OUTFILE_PATH: "."
        shell: bash -l {0}
        run: |
          test/run_notebook_tests.sh

      - name: Upload artifacts
        uses: actions/upload-artifact@v2
        with:
          name: wheel_lib
          path: |
            src/dist/*.whl
            src/main_notebook.py
            src/db_job.json
            environment.yml

  deploy:
    environment: Databricks_Azure_Production
    runs-on: ubuntu-latest
    if: github.event.pull_request.merged || github.event_name == 'workflow_dispatch'
    env:
      DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
      DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}

    steps:
      - name: Download artifacts
        uses: actions/download-artifact@v2
        with:
          name: wheel_lib

      - name: Setup Python
        uses: actions/setup-python@v2.2.2
        with:
          python-version: 3.7

      - name: Setup Miniconda
        uses: conda-incubator/setup-miniconda@v2.1.1
        with:
          environment-file: environment.yml
          activate-environment: databricks_cicd
          python-version: 3.7

      - name: Deploy lib and notebook to Databricks
        env:
          DATABRICKS_DBFS_PATH: dbfs:/mnt/databrickscicd/Production
          DATABRICKS_WORKSPACE_PATH: /databrickscicd/Production
        shell: bash -l {0}
        run: |
          name=$(cd src/dist; ls databrickscicd*.whl)
          databricks fs cp --overwrite src/dist/${name} ${DATABRICKS_DBFS_PATH}
          databricks workspace import --overwrite src/main_notebook.py --language PYTHON ${DATABRICKS_WORKSPACE_PATH}/main_notebook.py
          sed -i "s|__DATABRICKS_CICD_LIBRARY__|${DATABRICKS_DBFS_PATH}/${name}|g" src/db_job.json
          sed -i "s|__DATABRICKS_CICD_NOTEBOOK__|${DATABRICKS_WORKSPACE_PATH}/main_notebook.py|g" src/db_job.json
          databricks jobs create --json-file src/db_job.json
